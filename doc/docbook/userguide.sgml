<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook V4.1//EN">
<book lang="en">
	<bookinfo>
		<title>GridWay 5 Documentation: User Guide</title>
		<pubdate>May, 2006</pubdate>
		<copyright>
			<year>2002-2006</year>
			<holder>GridWay Team, Distributed Systems Architecture
				Group, Universidad Complutense de Madrid.</holder>
		</copyright>
		<legalnotice>
			<para>
				Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at
			</para>
			<para>
				<ulink url="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</ulink>
			</para>
			<para>
				Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
			</para>
			<para>
				Any academic report, publication, or other academic disclosure of results obtained with this Software will acknowledge this Software's use by an appropriate citation.
			</para>
      <para>
        GridWay is an effort undergoing incubation at Globus. Incubation is
        required of all newly accepted projects until a further review indicates
        that the infrastructure, communications, and decision making process
        have stabilized in a manner consistent with other successful Globus
        projects. While incubation status is not necessarily a reflection of
        the completeness or stability of the code, it does indicate that the
        project has yet to be fully endorsed by Globus.
      </para>
		</legalnotice>
	</bookinfo>


<chapter>
<title>Introduction to the GridWay Meta-Scheduler</title>

	<sect1>
	<title>Benefits for the End User </title>

  <para>
	GridWay, on top of Globus services, enables large-scale, secure and reliable
  sharing of computing resources (clusters, computing farms, servers,
  supercomputers...), managed by different resource management systems
  (PBS, SGE, LSF, Condor...), within a single organization (enterprise grid) or
  scattered across several administrative domains (partner or supply-chain
  grid). GridWay provides the end-user with a working environment and
  functionality similar to those found on local DRM systems, such as SGE, LSF or
  PBS. The end-user is able to submit, monitor and control his jobs by means of
  DRM-like commands (<command>gwsubmit</command>, <command>gwwait</command>,
  <command>gwkill</command>, <command>gwhosts</command>...) or the DRMAA API.
  In particular:
	</para>

  <para>
  <itemizedlist>
    <listitem>
    <para>
    Reliable and Unattended Execution of Jobs: Transparently to the end user,
    the scheduler is able to manage the different failure situations.
    </para>
    </listitem>

    <listitem>
    <para>
    Efficient Execution of Jobs: Jobs are executed on the faster available resources.
    </para>
    </listitem>

    <listitem>
    <para>
    Broad application scope: GridWay is not bounded to a specific class of
    application generated by a given programming environment and does not
    require application deployment on remote hosts, which extends its
    application range and allows reusing of existing software. GridWay allows
    Submission of single, array or complex jobs consisting of task dependencies,
    which may require file transferring and/or database access.
	  </para>
    </listitem>

    <listitem>
    <para>
    DRM Command Line Interface: The GridWay command line interface is similar
    to that found on Unix and resource management systems such as PBS or SGE.
    It allows users to submit, kill, migrate, monitor and synchronize jobs.
	  </para>
    </listitem>

    <listitem>
    <para>
    DRMAA Application Programming Interface: GridWay provides full support for
    DRMAA (GGF standard) to develop distributed applications (C and
    JAVA bindings).
	  </para>
    </listitem>
  </itemizedlist>
  </para>
	</sect1>

	<sect1>
	<title>How GridWay Operates</title>

  <para>
  GridWay enables you to treat your jobs as if they were Unix processes. Each
  job is given a numerical identifier, analogous to the PID of a process. This
  value is called the Job identifier, JID for short. If the job belongs to an
  array job, it will also have an array identifier, AID for short. A job's index
  within an array is called the task identifier, TID for short.
	</para>

  <para>
  Jobs are submitted using the <command>gwsubmit</command> command. A job is
  described by its template file. Here you can specify the job's executable
  file, its command line arguments, input/output files, standard stream
  redirection as well as other aspects.
	</para>

  <para>
  Jobs can be monitored using the <command>gwps</command> command. You can
  control your jobs at runtime using the <command>gwkill</command> command. You
  can synchronize your jobs using the <command>gwwait</command> command. You can
  find out what resources your job has used with the
  <command>gwhistory</command> command.
	</para>

  <para>
  System monitoring commands allow you to gather information of the GridWay
  system and the Grids you are using. These commands are:
  <command>gwuser</command> to show information about the users using GridWay;
  <command>gwhost</command> to monitor the  available hosts in the testbed; and
  <command>gwacct</command> to print usage (accounting) information per user or
  host.
	</para>

  <note>
  <para>
  Every command has a <option>-h</option> option which shows its usage and
  available options.
  </para>
  </note>
  </sect1>

	<sect1>
	<title>Job Life-Cycle in GridWay</title>

  <para>
  A job can be in one of the following dispatch states (DM state):
  <itemizedlist>
      <listitem>
      <para>
      Pending (<computeroutput>pend</computeroutput>): The job is waiting for
      a resource to run on. The job reaches this state when it is initially
      submitted by the user or when it is restarted after a failure, stop or
      self-migration.
      </para>
      </listitem>

      <listitem>
      <para>
      Hold (<computeroutput>hold</computeroutput>): The owner (or GridWay
      administrator) has held the job. It will not be scheduled until it
      receives a release signal.
      </para>
      </listitem>

      <listitem>
      <para>
      Prolog (<computeroutput>prol</computeroutput>): The job is preparing
      the remote system, by creating the execution directory in the remote host
      and transferring the input and restart (in case of migration) files to it.
      </para>
      </listitem>

      <listitem>
      <para>
      Pre-wrapper (<computeroutput>prew</computeroutput>): The job is making
      some advanced preparation tasks in the remote resource, like getting some
      data from a service, obtaining software licenses, etc.
      </para>
      </listitem>

      <listitem>
      <para>
      Wrapper (<computeroutput>wrap</computeroutput>): The job is executing the
      Wrapper, which in turns executes the actual application. It also starts
      a self-monitoring program if specified. This monitor, watches the
      raw performance (CPU usage) obtained by the application.
      </para>
      </listitem>

      <listitem>
      <para>
      Epilog (<computeroutput>epil</computeroutput>): The job is finalizing. In
      this phase it transfers the output and restart (in case of failure,
      stop or self-migration) files and cleaning up the remote system directory.
      </para>
      </listitem>

      <listitem>
      <para>
      Migration (<computeroutput>migr</computeroutput>): The job is migrating
      from one resource to another, by cancelling the execution of Wrapper and
      performing finalization tasks in the old resource (like in Epilog state)
      and preparation tasks in the new resource (like in Prolog state).
      </para>
      </listitem>

      <listitem>
      <para>
      Stopped (<computeroutput>stop</computeroutput>): The job is stopped. If
      restart files have been defined in the job template, they are transferred
      back to the client, and will be used when the job is resumed.
      </para>
      </listitem>

      <listitem>
      <para>
      Failed (<computeroutput>fail</computeroutput>): The job failed.
      </para>
      </listitem>

      <listitem>
      <para>
      Zombie (<computeroutput>zomb</computeroutput>): The job is done and the
      user can check the exit status.
      </para>
      </listitem>
    </itemizedlist>
		</para>

    <figure>
      <title>Simplified state machine of the GridWay Meta-scheduler.</title>
      <graphic align="center" fileref="../images/gw_states.jpg">
    </figure>

    <para>
    When a job is in Wrapper dispatch state, it can be in one of the following execution
    states (EM state), which are a subset of the available Globus GRAM states:
    <itemizedlist>

      <listitem>
      <para>
      Pending (<computeroutput>pend</computeroutput>): The job has been
      successfully submitted to the local DRM system and it is waiting for the
      local DRM system to execute it.
      </para>
      </listitem>

      <listitem>
      <para>
      Suspended (<computeroutput>susp</computeroutput>): The job has been
      suspended by the local DRM system.
      </para>
      </listitem>

      <listitem>
      <para>
      Active (<computeroutput>actv</computeroutput>): The job is being executed
      by the local DRM system
      </para>
      </listitem>

      <listitem>
      <para>
      Failed (<computeroutput>fail</computeroutput>): The job failed.
      </para>
      </listitem>

      <listitem>
      <para>
      Done (<computeroutput>done</computeroutput>): The job is done.
      </para>
      </listitem>

    </itemizedlist>
		</para>

		<para>
    Finally, The following flags are associated with a job (RWS flags):
      <itemizedlist>

      <listitem>
      <para>
      Restarted (<computeroutput>R</computeroutput>): Number of times the job
      was restarted or migrated.
      </para>
      </listitem>

      <listitem>
      <para>
      Waiting (<computeroutput>W</computeroutput>): Number of clients waiting
      for this job to end.
      </para>
      </listitem>

      <listitem>
      <para>
      Rescheduled (<computeroutput>S</computeroutput>): 1 if this job is waiting
      to be rescheduled, 0 otherwise.
      </para>
      </listitem>
      </itemizedlist>
		</para>
	</sect1>

	<sect1>
	<title>A Grid-Aware Application Model</title>

    <para>
    In order to obtain a reasonable degree of both application performance and
    fault tolerance, a job must be able to adapt itself according to the
    availability of the resources and the current performance provided by them.
    Therefore, the classical application model must be extended to achieve
    such functionality.
		</para>

		<para>
    The GridWay system assumes the following application model:
      <itemizedlist>

      <listitem>
      <para>
      <emphasis>Executable</emphasis>: The executable must be compiled for the remote host
      architecture. GridWay provides a straightforward method to select the
      appropriate executable for each host. The variable
      <varname>GW_ARCH</varname>, as provided by the Information MAD, can be
      used to define the executable in the job template (for example,
      EXECUTABLE=sim_code.${GW_ARCH})
      </para>
      </listitem>

      <listitem>
      <para>
      <emphasis>Input files</emphasis>: These files are staged to the remote host. GridWay provides a
      flexible way to specify input files and supports Parameter Sweep like
      definitions. Please note that these files may be also
      architecture dependent.
      </para>
      </listitem>

      <listitem>
      <para>
      <emphasis>Output files</emphasis>: These files are generated on the remote host and
      transferred back to the client once the job has finished.
      </para>
      </listitem>

      <listitem>
      <para>
      <emphasis>Standard streams</emphasis>: The standard input (STDIN) file is transferred to the
      remote system previous to job execution. Standard output (STDOUT) and
      standard error (STDERR) streams are also available at the client once
      the job has finished. These files could be extremely useful for debugging.
      </para>
      </listitem>

      <listitem>
      <para>
      <emphasis>Restart files</emphasis>: Restart files are highly advisable if dynamic scheduling
      is performed. User-level checkpointing managed by the programmer must be
      implemented because system-level checkpointing is not possible among
      heterogeneous resources.
      </para>
      </listitem>
  </itemizedlist>
	</para>

  <para>
  Migration is commonly implemented by restarting the job on the new candidate
  host. Therefore, the job should generate restart files at regular intervals
  in order to restart execution from a given point. However, for some
  application domains the cost of generating and transferring restart files
  could be greater than the saving in compute time due to checkpointing. Hence,
  if the checkpointing files are not provided the job is restarted from the
  beginning. In order not to reduce the number of candidate hosts where a job
  can migrate, the restart files should be architecture independent.
	</para>
  </sect1>
</chapter>

<chapter id='UserEnvConf'>
<title>User Environment Configuration</title>

  <important>
  <para>
    You should include the following environment variables in your shell
    configuration file. (example <filename>$HOME/.bashrc</filename>)
  </para>
  </important>


  <para>
  In order to set the user environment, follow these steps:
    <orderedlist>

      <listitem>
      <para>
      Set up Globus user environment:
      <screen>$ source $GLOBUS_LOCATION/etc/globus-user-env.sh</screen>
      </para>
      <para>
      or
      <screen>$ . $GLOBUS_LOCATION/etc/globus-user-env.csh</screen>
      </para>
      <para>
      depending on the shell you are using.
      </para>
      </listitem>

      <listitem>
      <para>
			Set up the GridWay user environment:
			<screen>
$ export GW_LOCATION=&lt;path_to_GridWay_installation>
$ export PATH=$PATH:$GW_LOCATION/bin</screen>
      </para>
      <para>
      or
      <screen>
$ setenv GW_LOCATION &lt;path_to_GW_location>
$ setenv PATH $PATH:$GW_LOCATION/bin</screen>
      </para>
      <para>
      depending on the shell you are using.
      </para>
      </listitem>

      <listitem>
      <para>
      Optionally, you can set up your environment to use the GridWay
      DRMAA library.
      <screen>$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$GW_LOCATION/lib</screen>
      </para>
      <para>
      or <screen>$ setenv LD_LIBRARY_PATH $LD_LIBRARY_PATH:$GW_LOCATION/lib</screen>
      </para>
      </listitem>

      <listitem>
      <para>
      Finally, if GridWay has been compiled with accounting support, you may
      need to set up the DB library. Example, if DB library has been installed in <filename>/usr/local/BerkeleyDB.4.4/</filename>:
      <screen>$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/BerkeleyDB.4.4/lib</screen>
      </para>
      <note>
      <para>
        This step is only needed if your environment has not been configured, ask your administrator.
      </para>
      </note>
      </listitem>
    </orderedlist>
	</para>
</chapter>

<chapter>
<title>Job Definition</title>

  <sect1>
	<title>Job Description Overview</title>

    <para>
      Job templates allow you to configure your job's requirements, in terms of
      needed files, generated files, requirements and ranks of execution hosts,
      as well as other options.
		</para>

    <para>
      Syntax:
      <screen>
&lt;VARIABLE> = ["]&lt;VALUE>["]
# &lt;Comments></screen>
		</para>

    <important>
    <para>
      Default values for EVERY job template are read from
      <filename>$GW_LOCATION/etc/job_template.default</filename>.
    </para>
  </important>
  </sect1>

  <sect1>
	<title>Job Template Options</title>

  <table frame='all'>
	<title>Job Template Options.</title>
    <tgroup cols='2' align='left' colsep='1' rowsep='1'>
    <colspec colname='c1' colwidth='6cm'>
    <colspec colname='c2'>
		<tbody>
		  <row>
			<entry namest="c1" nameend="c2">Execution</entry>
			</row>
        <row>
        <entry>EXECUTABLE</entry>
        <entry>The executable file. Example: EXECUTABLE = bin.${ARCH}</entry>
        </row>

        <row>
        <entry>ARGUMENTS</entry>
        <entry>Arguments to the above executable. Example:
        ARGUMENTS = "${TASK_ID}"</entry>
        </row>

        <row>
        <entry>ENVIRONMENT</entry>
        <entry>User defined, comma-separated, environment variables. Example:
        ENVIRONMENT = SCRATCH_DIR /tmp, LD_LIBRARY_PATH=/usr/local/lib</entry>
        </row>

      <row>
			<entry namest="c1" nameend="c2">I/O files</entry>
			</row>

        <row>
        <entry>INPUT_FILES</entry>
        <entry>A comma-separated pair of "local remote" filenames. If the remote
        filename is missing, the local filename will be preserved in the
        execution host. Example: INPUT_FILES = param.${TASK_ID} param,
        inputfile</entry>
        </row>

        <row>
        <entry>OUTPUT_FILES</entry>
        <entry>A comma-separated pair of remote filename local filename. If the
        local filename is missing, the remote filename will be preserved in the
        client host. Example: OUTPUT_FILES = outputfile, binary
        binary.${ARCH}.${TASK_ID}</entry>
        </row>

      <row>
			<entry namest="c1" nameend="c2">Standard streams</entry>
			</row>

        <row>
        <entry>STDIN_FILE</entry>
        <entry>Standard input file. Example: STDIN_FILE = /dev/null</entry>
        </row>

        <row>
        <entry>STDOUT_FILE</entry>
        <entry>Standard output file. Example:
        STDOUT_FILE = stdout_file.${JOB_ID}</entry>
        </row>

        <row>
        <entry>STDERR_FILE</entry>
        <entry>Standard error file. Example:
        STDERR_FILE = stderr_file.${JOB_ID}</entry>
        </row>

      <row>
			<entry namest="c1" nameend="c2">Checkpointing</entry>
      </row>

        <row>
        <entry>RESTART_FILES</entry>
        <entry>Checkpoint Files. These files are managed by the programmer and
        should be architecture independent (NO URLS HERE, you can use a
        checkpoint server using CHECKPOINT_URL). Example: RESTART_FILES = checkpoint</entry>
        </row>

        <row>
        <entry>CHECKPOINT_INTERVAL</entry>
        <entry>How often (seconds) restart files are transferred from the
        execution host to the checkpointing server.</entry>
        </row>

        <row>
        <entry>CHECKPOINT_URL</entry>
        <entry>GridFTP URL to store/access checkpoint files (Default is job
        experiment directory in localhost). Example:
        CHECKPOINT_URL = gsiftp://hydrus.ucm.es/var/checkpoints/</entry>
        </row>

      <row>
			<entry namest="c1" nameend="c2">Resource Selection
      (See <xref linkend="ResSelectExpr"> for more information)</entry>
			</row>

        <row>
        <entry>REQUIREMENTS</entry>
        <entry>A Boolean expression evaluated for each available host, if the
        evaluation returns true the host will be considered to submit the job.
        Example: REQUIREMENTS = ARCH = "i686" & CPU_MHZ > 1000;</entry>
        </row>

        <row>
        <entry>RANK</entry>
        <entry>A numerical expression evaluated for each candidate host (those
        for which the requirement expression is true). Those candidates with
        higher ranks are used first to execute your jobs. Example:
        RANK = (CPU_MHZ * 2) + FREE_MEM_MB;</entry>
        </row>

      <row>
		  <entry namest="c1" nameend="c2">Scheduling</entry>
			</row>

        <row>
        <entry>RESCHEDULING_INTERVAL</entry>
        <entry>How often GridWay searches the Grid for better resources to run
          this job. (0 = never)</entry>
        </row>

        <row>
        <entry>RESCHEDULING_THRESHOLD (seconds)</entry>
        <entry>If a better resource is found and the job has been running less
        than the threshold, it will migrate to the new host.</entry>
        </row>

      <row>
			<entry namest="c1" nameend="c2">Performance</entry>
			</row>

        <row>
        <entry>SUSPENSION_TIMEOUT</entry>
        <entry>Maximum suspension time (seconds) in the local job management
        system. If exceeded the job is migrated to another host.
        (0 = never)</entry>
        </row>

        <row>
        <entry>CPULOAD_THRESHOLD</entry>
        <entry>If the CPU assigned to your job is less than this given
        percentage, the job will be migrated</entry>
        </row>

        <row>
        <entry>MONITOR</entry>
        <entry>Optional program to monitor job performance</entry>
        </row>

      <row>
			<entry namest="c1" nameend="c2">Fault Tolerance</entry>
			</row>

        <row>
        <entry>RESCHEDULE_ON_FAILURE</entry>
        <entry>Behavior in case of failure. Possible values are 'yes' or
        'no'</entry>
        </row>

        <row>
        <entry>NUMBER_OF_RETRIES</entry>
        <entry>Number of retries in case of failure</entry>. GridWay follows a
        linear backoff strategy for re-trying file transfers and job
        submissions.
        </row>

			<row>
			<entry namest="c1" nameend="c2">Advanced Job Execution</entry>
			</row>

        <row>
        <entry>WRAPPER</entry>
        <entry>Script for wrapper. stdout and stderr streams of this program can
        be found in directory <filename>$GW_LOCATION/var/$GW_JOB_ID</filename>
        as files <filename>stdout.wrapper.$GW_RESTARTED</filename> and
        <filename>stderr.wrapper.$GW_RESTARTED</filename></entry>
        </row>

        <row>
        <entry>PRE_WRAPPER</entry>
        <entry>Optional program that is executed before the execution of the
        job, to perform an additional remote setup (e.g. access a web service).
        This job is ALWAYS submitted to the FORK job-manager. stdout and stderr
        streams of this program can be found in directory
        <filename>$GW_LOCATION/var/$GW_JOB_ID</filename> as files
        <filename>stdout.pre_wrapper.$GW_RESTARTED</filename> and
        <filename>stderr.pre_wrapper.$GW_RESTARTED</filename></entry>
        </row>

        <row>
        <entry>PRE_WRAPPER_ARGUMENTS</entry>
        <entry>Arguments to the pre-wrapper program.</entry>
        </row>
    </tbody>
		</tgroup>
	</table>
  </sect1>

  <sect1>
  <title>File Definition in Job Templates</title>

  <para>
  Input and output files are in general specified in a comma-separated,
  source/destination pair.
  </para>
  <screen>SRC1 DST1, SRC2 DST2,...</screen>
  <para>
  We next describe the available alternatives and protocols that you can use
  to choose the best staging strategy for your applications.
  </para>

    <sect2>
    <title> Defining Input Files </title>

    <para>
      Input files (if staged to the remote host) are always placed in the
      remote experiment directory. However you can specify the name used for
      the file in the remote directory with the destination name (DST above).
      This feature is very useful when your executable always expect a fixed
      input filename, and you want to process different input files (as is
      common in parametric computations).
    </para>

    <important>
      <para>
        The destination names for input files MUST be a single name, do not use
        absolute paths or URLs.
      </para>
    </important>

    <para>
    You can specify the input files using:
    <itemizedlist>
      <listitem>
        <para>
        <emphasis>Absolute Path</emphasis>: In this case no staging is
        performed. File is assumed to be in that location in the remote host.
        </para>
        <para>
        Example:
<screen>
EXECUTABLE  = /bin/ls      #Will use remote ls!</screen>
        </para>
      </listitem>

      <listitem>
        <para>
        <emphasis>GridFTP URL</emphasis>: The file will be downloaded from the
        given GridFTP url. If no destination is given the filename in the URL
        will be used in the remote host.
        </para>
        <para>
        Example, will copy file <filename>input_exp1</filename> from /tmp in
        machine to the remote host with name <filename>input</filename>.
        <screen>
INPUT_FILES  = gsiftp://machine/tmp/input_exp1 input</screen>
        </para>
      </listitem>

      <listitem>
        <para>
        <emphasis>File URL</emphasis>: The file will copied from an absolute
        path in the local host. If no destination is given the filename in the
        URL will be used in the remote host.
        </para>
        <para>
        Example:
        <screen>
INPUT_FILES  = file:///etc/passwd #Will copy local /etc/passwd file to remote dir, with name passwd</screen>
        </para>
      </listitem>

      <listitem>
        <para>
        <emphasis>Name</emphasis>: Use simple names to stage files in
        your local experiment directory (directory where the job template file
        is placed). If no destination is given the filename will be preserved.
        </para>
        <para>
        Example:
        <screen>
INPUT_FILES  = test_case.bin</screen>
        </para>
      </listitem>
    </itemizedlist>
    </para>
    <note>
    <para>
    The executable is treated as an input file, so the same remarks are
    applicable for the <parameter>EXECUTABLE</parameter> job template parameter.
    </para>
    </note>
    </sect2>

    <sect2>
      <title> Defining Output Files </title>

      <para>
        Output files are always copied FROM the remote experiment directory.
        However you can specify the destination of the output files of your
        applications.
      </para>

      <important>
        <para>
          The source names for output files MUST be a single name, do not use
          absolute paths or URLs.
        </para>
      </important>

      <para>
        You can specify the destination of output files using:
        <itemizedlist>
          <listitem>
            <para>
            <emphasis>Absolute Path</emphasis>: The remote file name will be
            copied to the absolute path in the local host. Note that you can
            also use <computeroutput>file:///</computeroutput> protocol.
            </para>
            <para>
            Example, to copy the output file <filename>output.bin</filename>
            in the <filename>/tmp</filename> directory of the local host with name
            <filename>outfile</filename>:
            <screen>
OUTPUT_FILES  = output.bin /tmp/outfile </screen>
            </para>
          </listitem>

          <listitem>
            <para>
              <emphasis>GridFTP URL</emphasis>: The file will be copied to the
              given GridFTP url.
            </para>
            <para>
              Example, you can also use variable substitution in URLs, see next
              section.
              <screen>
OUTPUT_FILES  = out gsiftp://storage_servere/~/output.${TASK_ID}</screen>
            </para>
          </listitem>

          <listitem>
            <para>
              <emphasis>Name</emphasis>: Use simple names to stage files to
              your local experiment directory (directory where the job template file
              is placed). If no destination is given the filename will be preserved.
            </para>
            <para>
              Example:
              <screen>
OUTPUT_FILES  = test_case.bin</screen>
            </para>
            <para>
              Tip: You can organize your output files in directories in the
              experiment directory (They MUST exist!), using a relative path
            <screen>
OUTPUT_FILES = outfile Out/file.${JOB_ID} #Directory Out must exist in the experiment directory</screen>
            </para>
          </listitem>
        </itemizedlist>
      </para>
  </sect2>

  <sect2>
      <title> Defining Standard Streams </title>
      <para>
        Standard streams includes the standard input for your executable
        (<parameter>STDIN_FILE</parameter>) and its standard output and error
        (<parameter>STDOUT_FILE</parameter> and <parameter>STDERR_FILE</parameter>).
      </para>
      <para>
        The STDIN_FILE can be defined using any of the methods described above
        for the input files. However you <emphasis> can not </emphasis> specifed
        a destination name for the standard input stream, as is internally handled
        by the system. Note also that <emphasis> only one</emphasis> standard input
        file can be specified.
      </para>
      <para>
        Example:
        <screen>
STDIN_FILE = In/input.${JOB_ID} #Will use input from directory In</screen>
      </para>
      <para>
        The STDOUT_FILE and STDERR_FILE parameters can be defined
        using any of the methods described above
        for the output files. However you <emphasis> can not </emphasis> specifed
        a source name (only destination).
        Note also that <emphasis> only one</emphasis> standard output and error
        files can be specified.
      </para>
      <para>
        Example:
        <screen>
          STDOUT_FILE = Out/ofile #Will place stdout in Out directory with name ofile</screen>
      </para>
    </sect2>

    <sect2>
    <title> Defining Restart Files </title>

      <para>
        Restart files are periodically copied to the job directory
        (<filename>$GW_LOCATION/var/$JOB_ID/</filename>). Restart files are
        only specified with its name. Note also that you can defined a
        checkpointing server with the <parameter>CHECKPOINT_URL</parameter>
        job template parameter.
      </para>
      <para>
        Example:
        <screen>
RESTART_FILES = tmp_file</screen>
      </para>
    </sect2>
  </sect1>

  <sect1 id='VarSub'>
	<title>Variable Substitution</title>

  <para>
	You can use variables in the value string of each option, with the format:
  <screen>${GW_VARIABLE}</screen>
	</para>

  <para>
	These variables are substituted at run time with its corresponding value. For example:
	</para>

  <screen>STDOUT_FILE = stdout.${JOB_ID}</screen>

  <para>
	will store the standard output of job 23 in the file <filename>stdout.23</filename>
	</para>

  <para>
	The following table lists the variables available to define job options,
  along with their description.
	</para>

  <table frame='all'>
    <title>Substitution Variables.</title>
		<tgroup cols='2' align='left' colsep='1' rowsep='1'>
		<colspec colname='c1' colwidth='6cm'>
		<colspec colname='c2'>
		<tbody>
		  <row>
			<entry>${JOB_ID}</entry>
			<entry>The job identifier</entry>
			</row>

      <row>
			<entry>${ARRAY_ID}</entry>
			<entry>The job array identifier (-1 if the job does not belong to any)</entry>
			</row>

      <row>
			<entry>${TASK_ID}</entry>
			<entry>The task identifier within the job array (-1 if the job does not
      belong to any)</entry>
			</row>

      <row>
			<entry>${TOTAL_TASKS}</entry>
			<entry>The total number of tasks in the job array (-1 if the job does not
      belong to any)</entry>
			</row>

      <row>
			<entry>${ARCH}</entry>
			<entry>The architecture of the selected execution host</entry>
			</row>
       <row>
			<entry>${PARAM}</entry>
			<entry>Allows the assignment of arbitrary start and increment values for array jobs
(start + increment*GW_TASK_ID). Useful to generate file naming patterns or task processing. The values for start and increment
will  be specified with options -s (for start, with 0 by default) and -i (for
increment, with 1 by default) of the gwsubmit command.</entry>
			</row>
       <row>
			<entry>${MAX_PARAM}</entry>
			<entry>Upper bound of the ${PARAM} variable.</entry>
			</row>
	  </tbody>
    </tgroup>
  </table>

  <important>
  <para>
    The above variables can be used in any job option.
  </para>
  </important>
  </sect1>

	<sect1 id="ResSelectExpr">
	<title>Resource Selection Expressions</title>

    <sect2>
    <title>Requirement Expression Syntax</title>
    <para>
    The syntax of the requirement expressions is defined as:
    <screen>
stmt::= expr';'
expr::= VARIABLE '=' INTEGER
        | VARIABLE '>' INTEGER
        | VARIABLE '&lt;' INTEGER
        | VARIABLE '=' STRING
        | expr '&' expr
        | expr '|' expr
        | '!' expr
        | '(' expr ')'</screen>
		</para>

    <para>
    Each expression is evaluated to 1 (TRUE) or 0 (FALSE). Only those hosts
    for which the requirement expression is evaluated to TRUE will be
    considered to execute the job.
    </para>
    <para>
      Logical operators are as expected ( less '&lt;', greater '>', '&' AND,
      '|' OR, '!' NOT), '=' means equals with integers. When you use '='
      operator with strings, it performs a shell wildcard pattern matching.
    </para>
    <para>
    Examples:
    <screen>
REQUIREMENTS = LRMS_NAME = "*pbs*"; # Only use pbs like jobmanagers
REQUIREMENTS = HOSTNAME = "*.es"; #Only hosts in Spain
REQUIREMENTS = HOSTNAME = "hydrus.dacya.ucm.es"; #Only use hydrus.dacya.ucm.es</screen>
    </para>
    </sect2>

    <sect2>
    <title>Rank Expression Syntax</title>

    <para>
    The syntax of the rank expressions is defined as:
    <screen>
stmt::= expr';'
expr::= VARIABLE
        | INTEGER
        | expr '+' expr
        | expr '-' expr
        | expr '*' expr
        | expr '/' expr
        | '-' expr
        | '(' expr ')'</screen>
    </para>

    <para>
    Rank expressions are evaluated using each host information. The
    '+','-','*','/', and '-' are arithmetic operators, so only integer values
    should be used in rank expressions.
    </para>

    <note>
    <para>
    Remember that the <parameter>nice</parameter> parameter for the information
    manager is added to the computed rank value.
    </para>
    </note>
    </sect2>

    <sect2>
    <title>Requirement and Rank Variables</title>

		<para>
    To set the <emphasis>REQUIREMENTS</emphasis> and <emphasis>RANK</emphasis>
    parameter values the following extended set of variables, provided by the
    Information Manager, can be used:
    </para>

    <table frame='all'>
	  <title>Variables that can be used to define the job <emphasis>REQUIREMENTS</emphasis> and <emphasis>RANK</emphasis>.</title>
		  <tgroup cols='2' align='left' colsep='1' rowsep='1'>
			<colspec colname='c1'>
			<colspec colname='c2'>
    	<tbody>
        <row>
		    <entry>HOSTNAME</entry>
			  <entry>FQDN (Fully Qualified Domain Name) of the execution host
        (e.g. "hydrus.dacya.ucm.es")</entry>
				</row>

        <row>
				<entry>ARCH</entry>
				<entry>Architecture of the execution host (e.g. "i686", "alpha")</entry>
    		</row>

        <row>
		    <entry>OS_NAME</entry>
			  <entry>Operating System name of the execution host (e.g. "Linux", "SL")</entry>
				</row>

        <row>
				<entry>OS_VERSION</entry>
				<entry>Operating System version of the execution host (e.g. "2.6.9-1.66", "3")</entry>
    		</row>

        <row>
		    <entry>CPU_MODEL</entry>
			  <entry>CPU model of the execution host (e.g. "Intel(R) Pentium(R) 4 CPU 2", "PIV")</entry>
				</row>

        <row>
				<entry>CPU_MHZ</entry>
				<entry>CPU speed in MHz of the execution host</entry>
    		</row>

        <row>
		    <entry>CPU_FREE</entry>
			  <entry>Percentage of free CPU of the execution host</entry>
				</row>

        <row>
				<entry>CPU_SMP</entry>
				<entry>CPU SMP size of the execution host</entry>
    		</row>

        <row>
		    <entry>NODECOUNT</entry>
			  <entry>Total number of nodes of the execution host</entry>
				</row>

        <row>
				<entry>SIZE_MEM_MB</entry>
				<entry>Total memory size in MB of the execution host</entry>
    		</row>

        <row>
		    <entry>FREE_MEM_MB</entry>
			  <entry>Free memory in MB of the execution hosts</entry>
				</row>

        <row>
    		<entry>SIZE_DISK_MB</entry>
	    	<entry>Total disk space in MB of the execution hosts</entry>
		    </row>

        <row>
				<entry>FREE_DISK_MB</entry>
				<entry>Free disk space in MB of the execution hosts</entry>
				</row>

        <row>
	    	<entry>LRMS_NAME</entry>
		    <entry>Name of local DRM system (job manager) for execution, usually
        not fork (e.g. "jobmanager-pbs", "Pbs", "jobmanager-sge", "SGE")</entry>
			  </row>

        <row>
				<entry>LRMS_TYPE</entry>
				<entry>Type of local DRM system for execution (e.g. "PBS", "SGE")</entry>
    		</row>

        <row>
		    <entry>QUEUE_NAME</entry>
			  <entry>Name of the queue (e.g. "default", "short", "dteam")</entry>
				</row>

        <row>
        <entry>QUEUE_NODECOUNT</entry>
        <entry>Total node count of the queue</entry>
		    </row>

        <row>
        <entry>QUEUE_FREENODECOUNT</entry>
        <entry>Free node count of the queue</entry>
				</row>

        <row>
	    	<entry>QUEUE_MAXTIME</entry>
		    <entry>Maximum wall time of jobs in the queue</entry>
			  </row>

        <row>
				<entry>QUEUE_MAXCPUTIME</entry>
				<entry>Maximum CPU time of jobs in the queue</entry>
    		</row>

        <row>
		    <entry>QUEUE_MAXCOUNT</entry>
        <entry>Maximum count of jobs that can be submitted in one request to
        the queue</entry>
				</row>

        <row>
				<entry>QUEUE_MAXRUNNINGJOBS</entry>
        <entry>Maximum number of running jobs in the queue</entry>
	    	</row>

        <row>
			  <entry>QUEUE_MAXJOBSINQUEUE</entry>
        <entry>Maximum number of queued jobs in the queue</entry>
				</row>

        <row>
	    	<entry>QUEUE_DISPATCHTYPE</entry>
        <entry>Dispatch type of the queue (e.g. "batch", "inmediate")</entry>
			  </row>

        <row>
    		<entry>QUEUE_PRIORITY</entry>
        <entry>Priority of the queue</entry>
		    </row>

        <row>
        <entry>QUEUE_STATUS</entry>
        <entry>Status of the queue  (e.g. "active", "production")</entry>
		    </row>
      </tbody>
	   	</tgroup>
    </table>
    </sect2>
	</sect1>

	<sect1>
	<title>Job Environment</title>

  <para>
  Job environment variables can be easily set with the <parameter>ENVIRONMENT
  </parameter> parameter of the job template. These environment variables are
  parsed, so you can use the GridWay variables defined in
  <xref linkend='VarSub'>, to set the job environment.
  </para>

  <note>
  <para>
  The variables defined in the ENVIRONMENT are "sourced" in a bash shell. In
  this way you can take advantage of the bash substitution capabilities and
  built-in functions. For example:
	</para>
  <screen>
ENVIRONMENT = VAR = "`expr ${JOB_ID} + 3`" # will set VAR to JOB_ID + 3</screen>
  </note>

  <para>
  In addition to those variables set in the <parameter>ENVIRONMENT
  </parameter> parameter, GridWay set the following variables, that can be used
  by your applications:
    <itemizedlist>
        <listitem>
          <para>GW_RESTARTED</para>
        </listitem>
        <listitem>
          <para>GW_EXECUTABLE</para>
        </listitem>
        <listitem>
          <para>GW_HOSTNAME</para>
        </listitem>
        <listitem>
          <para>GW_ARCH</para>
        </listitem>
        <listitem>
          <para>GW_CPU_MHZ</para>
        </listitem>
        <listitem>
          <para>GW_MEM_MB</para>
        </listitem>
        <listitem>
          <para>GW_RESTART_FILES</para>
        </listitem>
        <listitem>
          <para>GW_CPULOAD_THRESHOLD</para>
        </listitem>
        <listitem>
          <para>GW_ARGUMENTS</para>
        </listitem>
        <listitem>
          <para>GW_TASK_ID</para>
        </listitem>
        <listitem>
          <para>GW_CPU_MODEL</para>
        </listitem>
        <listitem>
          <para>GW_ARRAY_ID</para>
        </listitem>
        <listitem>
          <para>GW_TOTAL_TASKS</para>
        </listitem>
        <listitem>
          <para>GW_JOB_ID</para>
        </listitem>
        <listitem>
          <para>GW_OUTPUT_FILES</para>
        </listitem>
        <listitem>
          <para>GW_INPUT_FILES</para>
        </listitem>
        <listitem>
          <para>GW_OS_NAME</para>
        </listitem>
        <listitem>
          <para>GW_USER</para>
        </listitem>
        <listitem>
          <para>GW_DISK_MB</para>
        </listitem>
        <listitem>
          <para>GW_OS_VERSION</para>
        </listitem>
    </itemizedlist>
  </para>
  </sect1>
</chapter>

<chapter>
<title>Usage Scenarios</title>
  <sect1>
	<title>Single Jobs: Submitting and monitoring the simplest job</title>

  <para>
    GWD should be configured and running. Check the <emphasis>Installation and
    Configuration Guide</emphasis> and <xref linkend='UserEnvConf'> to do
    that. Do not forget to create a proxy with
    <command>grid-proxy-init</command>
  </para>

  <para>
  To submit a job, you will need a job template. The most simple job template
  in GridWay could be:
  <screen>
EXECUTABLE=/bin/ls</screen>
  </para>
  <para>
  Save it as file <filename>jt</filename> in directory <filename>example</filename>.
  </para>

  <para>
  Use the <command>gwsubmit</command> command to submit the job:
  <screen>
$ gwsubmit -t example/jt</screen>
	</para>

  <para>
  Let see how many resources are available in our Grid, with <command>gwhost</command>:
  </para>

  <screen>
HID HOSTNAME             OS              ARCH   MHZ %CPU  MEM(F/T)     DISK(F/T)  N(U/T) LRMS
0   ce00.inta.es         Scientific Linu i686  2800    0   513/513           0/0     0/7 jobmanager-lcgpbs
1   lcg2ce.ific.uv.es                SL3 i686  1200    0 1024/1024           0/0   0/126 jobmanager-pbs
2   ramses.dsic.upv.es   Scientific Linu i686   866    0   513/513           0/0    0/26 jobmanager-pbs
3   lcg-ce.usc.cesga.es  ScientificLinux i686  2500    0 1024/1024           0/0   0/100 jobmanager-lcgpbs
4   ursa.dacya.ucm.es    Linux2.6.12-1.1 x86   3201   44     7/470   24184/79561     0/2 Fork
5   hydrus.dacya.ucm.es  Linux2.6.10-1.7 i686  2539   97     4/503   12408/59399     0/4 PBS</screen>

  <para>
  Note also that we have Web Services resources (ursa and hydrus) and Pre-Web
  Services resources from EGEE. As we did not impose any requirements on the
  execution hosts all of them can be used to submit our job. You can also check
  the resources that match your requirements with <command>gwhost -m 0</command>.
  </para>

  <para>
  Now, you can check the evolution of the job with the <command>gwps</command>
  command.
  <screen width='80'>
JID AID TID DM   EM   RWS START    END      EXEC    XFER    EXIT TEMPLATE   HOST
0   --  --  pend ---- 000 17:08:51 --:--:-- 0:00:00 0:00:00 --   jt         --

JID AID TID DM   EM   RWS START    END      EXEC    XFER    EXIT TEMPLATE   HOST
0   --  --  prol ---- 000 17:08:51 --:--:-- 0:00:00 0:00:00 --   jt         hydrus.dacya.ucm.es/PBS

JID AID TID DM   EM   RWS START    END      EXEC    XFER    EXIT TEMPLATE   HOST
0   --  --  wrap ---- 000 17:08:51 --:--:-- 0:00:01 0:00:00 --   jt         hydrus.dacya.ucm.es/PBS

JID AID TID DM   EM   RWS START    END      EXEC    XFER    EXIT TEMPLATE   HOST
0   --  --  wrap pend 000 17:08:51 --:--:-- 0:00:02 0:00:00 --   jt         hydrus.dacya.ucm.es/PBS

JID AID TID DM   EM   RWS START    END      EXEC    XFER    EXIT TEMPLATE   HOST
0   --  --  wrap actv 000 17:08:51 --:--:-- 0:00:04 0:00:00 --   jt         hydrus.dacya.ucm.es/PBS

JID AID TID DM   EM   RWS START    END      EXEC    XFER    EXIT TEMPLATE   HOST
0   --  --  epil ---- 000 17:08:51 --:--:-- 0:00:05 0:00:01 --   jt         hydrus.dacya.ucm.es/PBS

JID AID TID DM   EM   RWS START    END      EXEC    XFER    EXIT TEMPLATE   HOST
0   --  --  zomb ---- 000 17:08:51 17:09:19 0:00:05 0:00:02 0    jt         hydrus.dacya.ucm.es/PBS</screen>
  </para>

  <para>
  At the beginning, the job is in <emphasis>pending</emphasis> state and not
  allocated to any resource. Then, the job is allocated to
  hydrus.dacya.ucm.es/PBS and begins the <emphasis>prolog</emphasis> stage.
  </para>

  <note>
  <para>
  You can use option <option>-c &lt;delay></option> to see a continuous
  output of the <command>gwps</command> command.
  </para>
  </note>

  <para>
  You can see the job history with the <command>gwhistory</command> command:
  <screen width='80'>
$ gwhistory 0
HID START    END      PROLOG  WRAPPER EPILOG  MIGR    REASON QUEUE    HOST
0   17:09:12 17:09:19 0:00:00 0:00:05 0:00:02 0:00:00 ----   default  hydrus.dacya.ucm.es/PBS</screen>
  </para>

  <para>
  Now it's time to retrieve the results. As you specified by default, the
  results of the execution of this job will be in the same folder, in a text
  file called sdtout_file.$JOB_ID.
  <screen>
$ ls -lt example/
total 48
-rw-r--r--  1 ehuedo staff    0 Feb 21 15:38 stderr_file.0
-rw-r--r--  1 ehuedo staff   34 Feb 21 15:38 stdout_file.0
-rw-r--r--  1 ehuedo staff   19 Feb 21 15:36 jt
$ cat example/stdout_file.0
stderr.execution
stdout.execution</screen>
  </para>

  <para>
  Done! You have done your first execution with GridWay!
  </para>
	</sect1>

  <sect1>
	<title>Array Jobs: Calculating the &pi; number</title>

    <sect2>
		<title>Defining the problem</title>

    <para>
    This is a well known exercise. For our purposes, we will calculate the
    integral of the following function:
      <informalfigure>
        <graphic align="center" fileref="../images/pi1.jpg">
      </informalfigure>
    </para>

    <para>
    Being f(x) = 4/(1+x2). So, &pi; will be the integral of f(x) in the
    interval [0,1].
   	</para>

    <para>
    In order to calculate the whole integral, it's interesting to divide the
    function in several sections and compute them separately:
    <informalfigure>
      <graphic align="center" fileref="../images/pi2.jpg">
      </informalfigure>
    </para>

    <para>
    As you can see, the more sections you make, the more exact &pi; will be:
    <informalfigure>
      <graphic align="center" fileref="../images/pi3.jpg">
    </informalfigure>
    </para>

    <para>
    So, you have a Grid with some nodes, you have GridWay... Why don't use them
    to calculate the &pi; number by giving all the nodes a section to compute
    with only one command?
    </para>

    <note>
    <para>
    You will find all the files needed to perform this example in the
    <filename>$GW_LOCATION/examples/pi</filename> directory.
    </para>
    </note>
		</sect2>

		<sect2>
		<title>The coding part</title>

    <para>
    For this example, we have chosen the C Programming Language. Create a text
    file called <filename>pi.c</filename> and copy inside the following lines:
    </para>

    <programlisting>
#include &lt;stdio.h>
#include &lt;string.h>

int main (int argc, char** args)
{
  int task_id;
  int total_tasks;
  long long int n;
  long long int i;

  double l_sum, x, h;

  task_id = atoi(args[1]);
  total_tasks = atoi(args[2]);
  n = atoll(args[3]);

  fprintf(stderr, "task_id=%d total_tasks=%d n=%lld\n", task_id, total_tasks, n);

  h = 1.0/n;

  l_sum = 0.0;

  for (i = task_id; i < n; i += total_tasks)
  {
    x = (i + 0.5)*h;
    l_sum += 4.0/(1.0 + x*x);
  }

  l_sum *= h;

  printf("%0.12g\n", l_sum);

  return 0;
}
  </programlisting>

    <para>
    Now it's time to compile it. We have chosen <command>gcc</command> for this
    purpose.
    <screen>$ gcc -O3 pi.c -o pi</screen>
    </para>

    <para>
    after this, you should have an executable called <command>pi</command>. This
    command receives three parameters:
    <itemizedlist>
      <listitem>
        <para>
          Task identifier: The identifier of the current task.
        </para>
      </listitem>
      <listitem>
        <para>
          Total tasks: The number of tasks the computation should be divided into.
        </para>
      </listitem>
      <listitem>
        <para>
          Number of intervals: The number of intervals over which the integral is being evaluated.
        </para>
      </listitem>
    </itemizedlist>
  </para>
  </sect2>

  <sect2>
	<title>Defining the Job</title>

  <para>
  For making GridWay work with your program, you must create a job template. In
  this case, we will call it <filename>pi.job</filename>. Copy the following
  lines inside:
  <screen>
EXECUTABLE  = pi
ARGUMENTS   = ${TASK_ID} ${TOTAL_TASKS} 100000
STDOUT_FILE = stdout_file.${TASK_ID}
STDERR_FILE = stderr_file.${TASK_ID}</screen>
  </para>
	</sect2>

  <sect2>
	<title>Submitting the jobs</title>

  <para>
  This time, we will submit an array of jobs. This is done by issuing the
  following command:
  <screen>
$ gwsubmit -v -t pi.job -n 4
ARRAY ID: 0

TASK JOB
0    0
1    1
2    2
3    3</screen>
    </para>

    <para>
    In order to wait for the jobs to complete, you can use the
    <command>gwwait</command> command.
    </para>

    <para>
    The argument passed to <command>gwwait</command> is the array identifier
    given by <command>gwsubmit</command> when executed with the
    <option>-v</option> option. It could be also obtained through
    <command>gwps</command>
    </para>

    <para>
    This command will block and return when all jobs have been executed:
    <screen>
$ gwwait -v -A 0
0   : 0
1   : 0
2   : 0
3   : 0</screen>
     This command, when issued with option <option>-v</option> shows the exit
     codes for each job in the array (usually, 0 means success).
    </para>
    </sect2>

    <sect2>
		<title>Result post-processing</title>

    <para>
    The execution of these jobs has returned some output files with the result
    of each execution:
    <screen>
stdout_file.0
stdout_file.1
stdout_file.2
stdout_file.3</screen>
    </para>

    <para>
    Now, we will need something to sum the results inside each file. For this,
    you can use an <command>awk</command> script like the following:
<screen width='80'>
$ awk 'BEGIN {sum=0} {sum+=$1} END {printf "Pi is %0.12g\n", sum}' stdout_file.*
Pi is 3.1415926536</screen>
    </para>

    <para>
    Well, not much precision, right? You could try it again, but this time with
    a much higher number of intervals (e.g. 10,000,000,000). Would you increment
    also the number of tasks? Which would be the best compromise?
    </para>

    <para>
    Do you imagine how easy would be to implement these steps in a shell script
    in order to perform them unattendedly? Here you are the prove:
    <programlisting>
#!/bin/sh

AID=`gwsubmit -v -t pi.job -n 4 | head -1 | awk '{print $3}'`

if [ $? -ne 0 ]
then
    echo "Submission failed!"
    exit 1
fi

gwwait -v -A $AID

if [ $? -eq 0 ]
then
    awk 'BEGIN {sum=0} {sum+=$1} END {printf "Pi is %0.12g\n", sum}' stdout_file.*
else
    echo "Some tasks failed!"
fi
    </programlisting>
    </para>
    </sect2>
  </sect1>

	<sect1>
	<title>Workflows</title>

  <para>
  The powerful commands provided by GridWay to submit, control and synchronize
  jobs allow us to programmatically define complex jobs or workflows, where some
  jobs need data generated by other jobs. GridWay allows job submission to be
  dependent on the completion of other jobs. This new functionality provides
  support for the execution of workflows.
	</para>

	<para>
  GridWay allows scientists and engineers to express their computational
  problems by using workflows. The capture of the job exit code allows users to
  define workflows, where each task depends on the output and exit code from
  the previous task. They may even involve branching, looping and spawning of
  subtasks, allowing the exploitation of the parallelism on the workflow of
  certain type of applications. The bash script flow control structures and the
  GridWay commands allow the development of workflows with the following
  functionality:
  <itemizedlist>

    <listitem>
      <para>
        Sequence, parallelism, branching and looping structures
      </para>
    </listitem>

    <listitem>
      <para>
        The workflow can be described in an abstract form without referring to
        specific resources for task execution
      </para>
    </listitem>

    <listitem>
      <para>
        Quality of service constraints and fault tolerance are defined at task
        level
      </para>
    </listitem>
  </itemizedlist>
	</para>

  <para>
  Job dependencies can be specified at submission by using the
  <command>-d</command> option of the <command>gwsubmit</command> command.
  A Job with dependencies will be submitted in the hold state, and once all
  the jobs on which it depends have successfully finished, it will
  be released. You can also release this job by hand with the <command>gwkill</command>.
  </para>

  <sect2>
  <title>A Sample of DAG Workflow</title>

  <para>
  A DAG-based workflow consists of a temporal relationship between tasks, where
  the input, output or execution of one ore more tasks depends on one or
  more other tasks. For this example we have chosen a simple workflow.
  </para>

  <figure>
    <title>Workflow example.</title>
    <graphic align="center" fileref="../images/gw_workflow.jpg">
  </figure>

  <para>
  In this example, job A generates a random number, jobs B and C add 1 to that
  number and, finally job D adds the result of these jobs. This is the final
  result is two times the number generated by A, plus two. In our case, the
  numbers are passed  between jobs using the standard output files.
  </para>
  <para>
  Job Template for job A (<filename>A.jt</filename>):
    <screen>
EXECUTABLE=/bin/echo
ARGUMENTS="$RANDOM"
STDOUT_FILE=out.A</screen>
  </para>

  <para>
  Job Template for jobs B and C (<filename>B.jt</filename> and
  <filename>C.jt</filename>):
  <screen>
EXECUTABLE=/usr/bin/expr
ARGUMENTS="`cat out.A`" + 1
INPUT_FILES=out.A
STDOUT_FILE=out.B #out.C for job C</screen>
  </para>

  <para>
  Job Template for job D (<filename>D.jt</filename>):
<screen>
EXECUTABLE=/usr/bin/expr
ARGUMENTS="`cat out.B`" + "`cat out.C`"
INPUT_FILES=out.B, out.C
STDOUT_FILE=out.workflow</screen>
  </para>

  <para>
  Once you have set up the previous job templates, the workflow can be easily
  submitted with the following commands:
  <screen>
$ gwsubmit -v -t A.jt
JOB ID: 5

$ gwsubmit -v -t B.jt -d "5"
JOB ID: 6

$ gwsubmit -v -t C.jt -d "5"
JOB ID: 7

$ gwsubmit -t C.jt -d "6 7"</screen>
  </para>
  <note>
  <para>
  In the previous example, jobs B and C can be submitted as an array
  job using just one template with output, <computeroutput>OUTPUT_FILES =
  out.${TASK_ID}</computeroutput>. Therefore, input of job D will be <computeroutput>
  INPUT_FILES = out.0, out.1</computeroutput>.
  </para>
  </note>
  <para>
  The above steps can be easily implemented in a shell script.
  <programlisting>
#!/bin/sh

A_ID=`gwsubmit -v -t A.jt | cut -f2 -d':' | cut -f2 -d' '`
B_ID=`gwsubmit -v -t B.jt -d "$A_ID" | cut -f2 -d':' | cut -f2 -d' '`
C_ID=`gwsubmit -v -t C.jt -d "$A_ID" | cut -f2 -d':' | cut -f2 -d' '`
D_ID=`gwsubmit -v -t D.jt -d "$B_ID $C_ID"| cut -f2 -d':' | cut -f2 -d' '`

#Sync with last job of the workflow
gwwait $D_ID

echo "Random number `cat out.A`"
echo "Workflow computation `cat out.workflow`"
  </programlisting>
  </para>

  <para>
  Note that when input and output files vary depending on the iteration or job id
  number, you should generate job templates dynamically before submitting each
  job. This can be done programmatically by using the DRMAA API, or via shell
  scripting.
  </para>
  </sect2>
  </sect1>
</chapter>

<chapter id='Troubleshooting'>
	<title>Troubleshooting</title>
	<sect1>
	<title>Debugging Job Execution</title>

  <para>
  GridWay reporting and accounting facilities provide information about
  overall performance and help debug job execution. GWD generates the following
  files under the <filename>$GW_LOCATION/var</filename> directory:
  <itemizedlist>
    <listitem>
      <para>
        <filename>gwd.log</filename>: System level log. You can find log
        information of the activity of the middleware access drivers; and a
        coarse-grain log information about jobs.
      </para>
    </listitem>
    <listitem>
      <para>
        <filename>$JOB_ID/job.log</filename>: Detailed log information for
        each job, it includes details of job state transitions, resource usage
        and performance.
      </para>
    </listitem>
    <listitem>
      <para>
        <filename>$JOB_ID/stdout.wrapper</filename>: Standard output of the
        wrapper executable.
      </para>
    </listitem>
    <listitem>
      <para>
        <filename>$JOB_ID/stderr.wrapper</filename>: Standard error output of
        the wrapper executable. By default, wrapper is executed with shell
        debugging options (<option>-xv</option>) active, so this is usually the
        best source of information in case of failure.
      </para>
    </listitem>
  </itemizedlist>
  </para>
  </sect1>

  <sect1>
	<title>Frequent Problems</title>

  <para>
	Currently, many errors are handled silently and are only shown in the
  <filename>job.log</filename> file. Mainly sintax errors in requirement and
  rank expressions.
  </para>

	<para>
  Also, there is a number of failures related to the underlying middleware
  (Globus in this case) that could make some jobs fail. It is a good idea to
  perform some basic testing of Globus when some jobs unexpectedly fail
  (see the <emphasis>Installation and Configuration Guide</emphasis> to learn
  how to verify a Globus installation)
	</para>

  <para>
  Use the GridWay forum, in <filename>www.gridway.org/forum</filename>, to
  submit your problems and they will be eventually appear in this guide.
	</para>
	</sect1>
	</chapter>
</book>




